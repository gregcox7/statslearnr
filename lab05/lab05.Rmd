---
title: "Hypothesis Testing Using Randomization"
output:
    learnr::tutorial:
        progressive: true
        allow_skip: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(tidyverse)
library(infer)
library(learnr)
library(gradethis)

tutorial_options(
    exercise.checker = gradethis::grade_learnr,
    exercise.completion = FALSE
)
gradethis_setup(
    exercise.reveal_solution = FALSE
)

kobe <- read_csv("data/kobe.csv")
asc_choice <- read_csv("data/asc_choice_job.csv")

set.seed(12222)

asc_null_distribution <- asc_choice %>%
    specify(Choice ~ Group, success = "Consistent") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "diff in props", order = c("ASC", "NT"))

asc_obs_diff <- asc_choice %>%
    specify(Choice ~ Group, success = "Consistent") %>%
    calculate(stat = "diff in props", order = c("ASC", "NT"))
```

## Introduction

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/kobe.png")
```

In this session, we will get some initial practice in testing hypotheses by randomization.  While this practice will not cover all the nuances of hypothesis testing in statistics, it will touch on many of the key ideas that we will see in different forms throughout the rest of the course.

In the first part, we will get a sense of how **permutation** works, and how it helps us to simulate what a sample might look like if the null hypothesis were true.  Permutation is one way that we can use "randomization" to test a null hypothesis.  In the second part, we will use permutation to test a more serious hypotheses about how different types of people might make decisions differently.

## Did Kobe have a hot hand?

To get a handle on some of the big ideas in hypothesis testing in general, and of permutation in particular, let's first look at a very special dataset.  These data pertain to Kobe Bryant of the LA Lakers playing against the Orlando Magic in the 2009 NBA finals.  Commentators at the time remarked that Kobe seemed to have a "hot hand".  In other words, they were claiming that once Kobe made a basket, he was *more likely* to make a basket on his next shot.

### Check out the data

We have loaded into R a dataset called `kobe` that consists of every shooting attempt Kobe made during that game, including whether or not it went in (i.e., was the shot a "Hit" or a "Miss"):

```{r}
head(kobe)
```

### Framing the hypotheses

We can translate the claim of the hot hand into a **null hypothesis** and an **alternative hypothesis**.  For us to believe the "hot hand" claim, we first have to rule out the possibility that Kobe's hit proportion is the same regardless of whether his previous shot went in or not.  This possibility is the **null hypothesis**.  The **alternative hypothesis** is that Kobe really had a hot hand and he made a greater proportion of hits after having already made a hit than after missing.

### Summarize the data

Now that we've framed our hypotheses, let's see whether our data suggest that we could reject the null hypothesis.  To begin, we will use a frequency table:

```{r}
kobe %>%
    group_by(prev_shot, shot) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

Here, `prev_shot` refers to whether Kobe's `prev`ious shot was a hit ("prev_H") or a miss ("prev_M").  The final column `p` gives us the proportions of Hits ("H") or Misses ("M") following either a Previous Hit or Previous Miss.

---

**Exercise 1**

```{r kobe_calc, exercise = TRUE, exercise.eval = FALSE, exercise.lines = 1}

```

```{r kobe_calc-check}
grade_result(
  pass_if(~identical(round(.result, 2), round(18 / (18 + 32) - 25 / (25 + 36), 2)))
)
```

a. Based on the proportions in the table above, what is the difference between Kobe's proportion of *Hits following a Previous Hit* minus his proportion of *Hits following a Previous Miss*?  You can use the empty R box above to both calculate and check your answer.
b. Given your answer in (a), could these data be consistent with Kobe having a "hot hand"?  Explain your reasoning.

---

### Simulating a possible dataset if the null were true

If the null hypothesis were true, whether Kobe does or does not make a shot should not depend on whether he did or did not make his previous shot.  We can *model* what Kobe's performance would look like if this null hypothesis were true.  To do this, we treat Kobe's shooting history as if it were random.  If the null hypothesis were true, we should be able to randomly **permute** Kobe's shooting history *without changing the overall relationship between Kobe's previous shots and his current shot*.  By "overall relationship", we mean the *difference in proportions* that we just calculated.  If the null hypothesis were true, the difference in proportions from our permuted data should not be *too different* from the difference in proportions in our actual data.  On the other hand, if the null hypothesis were false, the difference in proportions in the permuted data should be very different from what we actually saw.

We can use R to permute Kobe's shot history and thereby model how Kobe's shots *might* have gone if the null hypothesis were true.  To do this, we need to `specify` the relevant variables as well as the `hypothesis`.  Let's see that all at once, then unpack it:

```{r}
kobe_simulated <- kobe %>%
    specify(shot ~ prev_shot, success = "H") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1, type = "permute")
```

In the first line, the phrase `kobe_simulated <- ` tells R to store the result under the name `kobe_simulated`.

In the second line, we `specify` which variable is the **explanatory variable** (`prev_shot`) and which variable is the **response variable** (`shot`).  We do this using the same squiggly thing we used for linear regression (`[response variable] ~ [explanatory variable]`).  Because the response variable is a binary (i.e., two-level) categorical variable, we also need to tell R that a "success" is a Hit (abbreviated "H").

The third line is how we tell R what our null hypothesis is, namely, that the explanatory and response variables are **independent** (not associated).

Finally, the fourth line `generate`s a simulated dataset by randomly permuting---that is, *shuffling*---the columns of the original data containing the explanatory and response variables.

---

**Exercise 2**

Find the same difference in proportions that you found in the previous exercise, but for the *simulated* data we just created.  Use the first code box below to create a summary table like the one we made for Kobe's actual data (again, you can check your work with "Submit Answer").  Use the second code box to calculate the difference in proportions (again, you can check your work with "Submit Answer").

```{r kobe_sim_summary, exercise = TRUE, exercise.eval = FALSE, exercise.lines = 4}

```

```{r kobe_sim_summary-solution}
# order 1 ----
kobe_simulated %>%
    group_by(prev_shot, shot) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))

# order 2 ----
kobe_simulated %>%
    group_by(shot, prev_shot) %>%
    summarize(n=n()) %>%
    mutate(p = n / sum(n))
```

```{r kobe_sim_summary-check}
grade_this_code()
```

```{r kobe_sim_calc, exercise = TRUE, exercise.eval = FALSE, exercise.lines = 1}

```

```{r kobe_sim_calc-check}
grade_result(
  pass_if(~identical(round(.result, 2), round(kobe_simulated %>% specify(shot ~ prev_shot, success = "H") %>% calculate(stat = "diff in props", order = c("prev_H", "prev_M")) %>% pull(), 2)))
)
```

Compare the difference you just calculated (from simulated data) to the difference you calculated from the actual data in the previous exercise.  Do they seem similar or different?  Speculate about what this might tell us about whether the null hypothesis is true or not.

---

### Simulating many possible datasets if the null were true

All we did was simulate *one* possible way Kobe's shots could have turned out if the null hypothesis were true.  There are many possible ways Kobe's shots could have turned out, and each of these corresponds to a different random permutation of Kobe's shot history.  It's a good thing that computers are good at repetitive tasks, because we can use the computer to repeat the random permutation process many times to simulated many possible datasets.

The next chunk of code generates 1000 different simulated datasets by randomly permuting Kobe's shot history.  We tell R to remember these all under the name `kobe_many_simulations`.

```{r}
kobe_many_simulations <- kobe %>%
    specify(shot ~ prev_shot, success = "H") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute")
```

Here's a glimpse of the result:

```{r}
head(kobe_many_simulations)
```

The "replicate" column is a number from 1 to 1000 that labels each simulated dataset.  But this is still not super helpful, since we have to calculate the difference in proportions for each of those thousand simulated datasets.  Fortunately, R gives us an easy way to do that by adding a line called `calculate` at the end of our chunk:

```{r}
kobe_null_distribution <- kobe %>%
    specify(shot ~ prev_shot, success = "H") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "diff in props", order = c("prev_H", "prev_M"))
```

We have now labeled the result `kobe_null_distribution`, and the result looks like this:

```{r}
head(kobe_null_distribution)
```

Again, the "replicate" column is a label for each simulated dataset, and the new "stat" column is the difference $\hat{p}_{\text{Prev. H}} - \hat{p}_{\text{Prev. M}}$ *for each simulated dataset*.

We can now make a histogram to examine the *distribution of differences in proportions that result when the null hypothesis is true*:

```{r}
kobe_null_distribution %>%
    visualize()
```

---

**Exercise 3**

Describe the distribution of differences in proportions when the null hypothesis is true.

a. How many modes does the distribution have, and does it seem to be skewed at all?
b. Roughly where does the center of the distribution fall?
c. Roughly where do the lower and upper ends of the distribution seem to fall?

---

### Find the $p$ value

Remember that the $p$ value is the proportion of simulated datasets which are *at least as extreme* as our actual data.  In this case, that means the proportion of simulated datasets with a difference in proportions that is *bigger* than what we observed.  Let's tell R to remember that difference by running the chunk of code below:

```{r}
kobe_obs_diff <- kobe %>%
    specify(shot ~ prev_shot, success = "H") %>%
    calculate(stat = "diff in props", order = c("prev_H", "prev_M"))
```

Now, let's see how many simulated datasets were bigger than this.  In the following line of code, we have to tell R that the observed "statistic" (`obs_stat`) is the difference we just told it to remember (`kobe_obs_diff`) as well as the fact that we are interested in how many simulations produced results that were "greater" than what was observed:

```{r}
kobe_null_distribution %>%
    get_p_value(obs_stat = kobe_obs_diff, direction = "greater")
```

Finally, it will help to visualize where the observed difference falls relative to the distribution of differences from our simulated data (note that this uses `shade_p_value` rather than `get_p_value`):

```{r}
kobe_null_distribution %>%
    visualize() +
    shade_p_value(obs_stat = kobe_obs_diff, direction = "greater")
```

The red line is the value that was actually observed and the parts of the histogram that are shaded pink represent the simulated datasets that were "more extreme" than what was observed.

### Form a conclusion

Remember that we were testing the **null hypothesis** that Kobe did *not* have a hot hand.  We only reject the null hypothesis if our data would be very unlikely if the null hypothesis were true, i.e., if the $p$ value is low.  Assume we adopt a **significance level** of 0.05, so we would reject the null hypothesis if the $p$ value were less than this level.

---

**Exercise 4**

Do you reject the null hypothesis?  Explain your reasoning.  What does your conclusion say about whether or not Kobe had a "hot hand"?

---

## Do people on the autism spectrum make more consistent choices?

The kind of hypothesis testing we just did for the fun case of Kobe's hot hand is the same as what we use to answer more serious research questions.

Autism is a condition that has many facets.  Individuals with autism who do not have any cognitive impairments often have different cognitive "styles".  In particular, it is thought that people with autism are more "detail-oriented".  This can be a detriment when trying to find a general pattern, but it might be a benefit in situations where there are many irrelevant distractions.

This potential benefit was studied by @FarmerEtAl2017.  Their experiment included a group of participants diagnosed as being on the autism spectrum (with no cognitive impairments) as well as a group of neuro-typical controls.  They looked at participants' choices between pairs of consumer products that were presented alongside a third, less desirable "decoy" option.  A "rational" choice should not be affected by the presence of this decoy, but in fact people are often swayed by those irrelevant options.  Might people with autism make choices that are more consistent---more "rational"---because they ignore the irrelevant decoy?

For the rest of the lab, you will conduct a hypothesis test to address this question.  It will follow the same basic outline as the procedure we followed to address the "hot hand" question, so be sure to refer to the previous section for guidance.

### Check out the data

The relevant data are stored in R under the name `asc_choice`.  Here's what the first few rows look like.

```{r}
head(asc_choice)
```

The **response variable** is `Choice`, which is either "Consistent" (if the participant's choice was not affected by the decoy) or "Inconsistent" (if the participant's choice was affected by the decoy).

The **explanatory variable** is `Group`, which is either "ASC" (for "Autism Spectrum Condition") or "NT" (for "Neuro-Typical" control).

### Framing the hypotheses

Our **research question** is, "is the proportion of consistent choices *higher* for participants from the ASC group than those from the NT group?"

---

**Exercise 5**

What are the **null hypothesis** and **alternative hypothesis** that correspond to this research question?

---

### Summarize the data

The next step is to construct a frequency table so we can know what the proportions of consistent choices were for each group and then find the difference.

---

**Exercise 6**

Find the proportions of consistent choices for each group, so we can find the difference between them (as usual, you can check your work using "Submit Answer").

```{r asc_summary, exercise = TRUE, exercise.eval = FALSE}
___ %>%
    group_by(___, ___) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

```{r asc_summary-solution}
asc_choice %>%
    group_by(Group, Choice) %>%
    summarize(n = n()) %>%
    mutate(p = n / sum(n))
```

```{r asc_summary-check}
grade_this_code()
```

What is the difference in proportion of consistent choices between the autism spectrum group (ASC) and neuro-typical control group (NT)?  Is this difference more consistent with the null hypothesis or with the alternative hypothesis?  (You can use the box below to check your answer.)

```{r asc_calc, exercise = TRUE, exercise.eval = FALSE, exercise.lines = 1}

```

```{r asc_calc-check}
grade_result(
  pass_if(~identical(round(.result, 2), round(asc_choice %>% specify(Choice ~ Group, success = "Consistent") %>% calculate(stat = "diff in props", order = c("ASC", "NT")) %>% pull(), 2)))
)
```

---

### Model the null hypothesis

If the null hypothesis were true, a participant's choice shouldn't depend on which group they are in.  In the Kobe example, we modeled the null hypothesis by randomly permuting Kobe's shot history, since that was the explanatory variable.  In this research scenario, group is the explanatory variable.  So by randomly shuffling participants between groups, we can simulate how the data would look if the null hypothesis were true.

---

**Exercise 7**

Fill in the blanks in the code below to simulate 1000 datasets assuming the null hypothesis were true, then produce a histogram (using the `visualize` function) of the resulting simulated differences in proportions.  For guidance, check out how we did it in the Kobe example above.  Be sure to note:

* What is the name of the relevant dataset?
* What are the names of the explanatory and response variables?
* A "Consistent" choice counts as a "success".
* We want to do *1000* simulations.
* For the difference in proportions, we want to look at ASC minus NT.

```{r asc_null, exercise = TRUE, exercise.eval = FALSE}
asc_null_distribution <- ___ %>%
    specify(___ ~ ___, success = "___") %>%
    hypothesize(null = "___") %>%
    generate(reps = ___, type = "permute") %>%
    calculate(stat = "___", order = c("___", "___"))

asc_null_distribution %>%
    visualize()
```

```{r asc_null-solution}
asc_null_distribution <- asc_choice %>%
    specify(Choice ~ Group, success = "Consistent") %>%
    hypothesize(null = "independence") %>%
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "diff in props", order = c("ASC", "NT"))

asc_null_distribution %>%
    visualize()
```

```{r asc_null-check}
grade_this_code()
```

Once your code is working correctly, try clicking "Run Code" or "Submit Answer" a few times.  a. Explain in your own words why the resulting distribution does not look the same each time.
b. Based just on looking at the histogram (no need to do any calculations), describe what aspects of the distribution seem to stay the same and which seem to differ each time you run your code.  Note things like the shape of the distribution (number of modes and skewness), central tendency, and variability.

---

### Find the $p$ value

To find the $p$ value, we first need to get the observed difference in proportions and tell R to remember it with the label `asc_obs_diff`.

---

**Exercise 8**

Fill in the blanks below to find the observed difference in proportions in the actual data and tell R to remember it under the label `asc_obs_diff`.  This value is then used to calculate the $p$ value.  *Hint 1:* for the `direction`, remember that this should correspond to the alternative hypothesis ("less", "greater", or "two-sided").  *Hint 2:* see what you wrote in the previous exercise, there's a lot you can reuse!

```{r asc_obs_calc, exercise = TRUE, exercise.eval = FALSE}
asc_obs_diff <- ___ %>%
    specify(___ ~ ___, success = "___") %>%
    calculate(stat = "___", order = c("___", "___"))

asc_null_distribution %>%
    get_p_value(obs_stat = asc_obs_diff, direction = "___")
```

```{r asc_obs_calc-solution}
asc_obs_diff <- asc_choice %>%
    specify(Choice ~ Group, success = "Consistent") %>%
    calculate(stat = "diff in props", order = c("ASC", "NT"))

asc_null_distribution %>%
    get_p_value(obs_stat = asc_obs_diff, direction = "greater")
```

```{r asc_obs_calc-check}
grade_this_code()
```

---

### Form a conclusion

Finally, we are in a position to visualize where the observed difference in proportions falls relative to the differences that would be expected if the null hypothesis were true.  This will enable us to form a conclusion about what these data tell us about our research question.

---

**Exercise 9**

Fill in the blanks in the code below to visualize where the observed difference in proportions falls on the distribution of differences that would be expected if the null hypothesis were true (*hint:* look back at previous exercises, you'll be able to re-use a lot!)

```{r asc_null_vis, exercise = TRUE, exercise.eval = FALSE}
___ %>%
    visualize() +
    shade_p_value(obs_stat = ___, direction = "___")
```

```{r asc_null_vis-solution}
asc_null_distribution %>%
    visualize() +
    shade_p_value(obs_stat = asc_obs_diff, direction = "greater")
```

```{r asc_null_vis-check}
grade_this_code()
```

Based on our analyses, would you reject the null hypothesis?  Explain your reasoning, being sure to choose a reasonable *significance level*.  What does your conclusion say about whether participants with a diagnosis of autism made more consistent choices than neuro-typical participants?

---

## Wrap-up

In this session, we got practice using R to perform hypothesis tests using randomization.  Specifically, we used a type of shuffling called permutation.  Shuffling allows us to simulate various ways that a particular dataset could look like if the null hypothesis were true.  We then found the $p$ value and visualized the null distribution in order to get a sense of whether our actual data would be unlikely if the null hypothesis were true.