---
title: "Sampling"
output:
    learnr::tutorial:
        progressive: true
        allow_skip: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(tidyverse)
library(learnr)
library(gradethis)

tutorial_options(
    exercise.checker = gradethis::grade_learnr,
    exercise.completion = FALSE
)
gradethis_setup(
    exercise.reveal_solution = FALSE
)
berkeley <- read_csv("data/berkeley.csv")
```

## Introduction

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/simpsons.png")
```

In the previous activity, we got some exposure to how we can explore data in R in such a way that we can use data to help answer questions.  In this activity, we will get a sense of where data come from and how the processes that produce data can force us to change how we interpret that data.

Generally, the data we have come from a **sample** from some larger **population**.  Depending on how that sample is selected, it may give us a **biased** perspective on the larger population.  In particular, when we have non-random samples (as in **observational studies**), we must think carefully about how the processes by which cases end up in our sample.  If those processes, which might be social in nature, introduce **confounding variables**, then there can be systematic differences between different groups in our sample, requiring us to interpret the data differently.

## Sex Bias, Sampling Bias, or Both?

In 1973, the University of California at Berkeley noted a disturbing fact about their graduate school admissions: considerably more male applicants were being admitted than female applicants.  The University, fearing a lawsuit, wanted to know if this reflected systematic sex discrimination in their admissions offices.  We shall see for ourselves that the story turned out to be more complex (for the full details, check out the [original article](https://www.jstor.org/stable/1739581)).  For now, keep in mind that this sample is *not random* because applicants, by definition, select themselves into the sample.

### Check out the data

Let's first take a look at the raw data.  These are loaded into R under the name `berkeley`.

```{r}
head(berkeley)
```

Each row of the data refers to a specific applicant in 1973.  For each applicant, there are three observed variables:

* **Admit**: Either "Admitted" or "Rejected", depending on whether the applicant was admitted or rejected.
* **Gender**: Either "Male" or "Female".
* **Department**: A letter between "A" and "F" that identifies the academic department to which the applicant applied.  The actual department names were obscured for privacy.

---

**Exercise 1**

For each variable in this dataset, identify what type it is (one of nominal categorical, ordinal categorical, numerical).

---

### Are more males than females admitted?

As noted above, Berkeley was concerned because a greater **proportion** of their male applicants were being admitted than their female applicants.  To verify whether this is true, let's find those proportions ourselves by running the following chunk of code:

```{r}
berkeley %>%
  group_by(Gender, Admit) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

The rightmost column of the table (labeled `p`) gives the proportion of applicants of each gender who were either admitted or rejected.  Based on this table, it looks like Berkeley was right to be concerned---the proportion of male applicants admitted is higher than the proportion of female applicants admitted.

---

**Exercise 2**

Consider the structure of the chunk of code we just ran, which is identical to how we made tables of frequencies and proportions in the previous activity.

```{r eval=FALSE}
___ %>%
  group_by(___) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

For each of the four lines of code above, describe in your own words what is the purpose of that line.

---

### Is there a bias in all departments?

Even if there is an overall bias, is this the case for all departments, or just some?

---

**Exercise 3**

Make a new table that gives the proportion of male and female applicants admitted for each department.  You may find it helpful to fill in the blanks in the following chunk of code (note a new line at the end which "filters" out some redundant rows of the table to make it from being too long).  Remember that you can use the "Submit Answer" button to check your work.

```{r department_admit, exercise = TRUE, exercise.eval = FALSE}
___ %>%
  group_by(___, ___, Admit) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n)) %>%
  filter(Admit == "Admitted")
```

```{r department_admit-solution}
# department then gender ----
berkeley %>%
  group_by(Department, Gender, Admit) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n)) %>%
  filter(Admit == "Admitted")

# gender then department ----
berkeley %>%
  group_by(Gender, Department, Admit) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n)) %>%
  filter(Admit == "Admitted")
```

```{r department_admit-check}
grade_this_code()
```

Based on the table you just made, which departments admit a higher proportion of female applicants and which admit a higher proportion of male applicants?

---

### Resolving the paradox: Tables

We appear to have a paradox on our hands:  If we ignore differences between departments, it is more likely that a male applicant gets admitted than a female applicant.  But within most departments, the opposite is true: a greater proportion of female applicants are admitted than male applicants.  In other words, there seems to be a **confounding variable** at work, and it is related to the differences between departments.

To try to figure out what this confound might be, we should look at two additional issues.  The first issue is, for each department, what proportion of their applicants are female, regardless of whether they were admitted or not?

---

**Exercise 4**

Modify the following chunk of code to produce a table that gives the proportion of applicants for each department who were either male or female.

```{r gender_apply, exercise = TRUE, exercise.eval = FALSE}
___ %>%
  group_by(___, ___) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

```{r gender_apply-solution}
# department then gender ----
berkeley %>%
  group_by(Department, Gender) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))

# gender then department ----
berkeley %>%
  group_by(Gender, Department) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

```{r gender_apply-check}
grade_this_code()
```

Based on the table you just produced, which departments have more male than female applicants?  Is there any overlap between these departments and those you identified in the previous exercise as admitting a higher proportion of female than male applicants?

---

The second issue to consider is the overall rate of admission for each department.

---

**Exercise 5**

Make a table that shows the proportion of applicants who were admitted for each department, regardless of gender.  You may find it useful to modify the chunk of code you just used.

```{r admit_apply, exercise = TRUE, exercise.eval = FALSE}
___ %>%
  group_by(___, ___) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

```{r admit_apply-solution}
# department then gender ----
berkeley %>%
  group_by(Department, Admit) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))

# gender then department ----
berkeley %>%
  group_by(Admit, Department) %>%
  summarize(n = n()) %>%
  mutate(p = n / sum(n))
```

```{r admit_apply-check}
grade_this_code()
```

For each department, compare the proportion of applicants admitted to the proportion of applicants who are female.  Describe any relationship between those two proportions that you might see.

---

### Resolving the paradox: Bar charts

Finally, let's create a visualization that may help us put these pieces together.  The chunk of code below produces a set of bar charts, one for each department.  These are similar to those we made in the previous activity, but have a new "aesthetic", namely, they use a variable (`Gender`) to `fill` in the bars with different colors:

```{r}
berkeley %>%
  ggplot(aes(x = Admit, fill = Gender)) +
  geom_bar() +
  facet_wrap("Department")
```

---

**Exercise 6**

Compare the code we just ran to create a colored bar chart with this chunk of code we used to make a bar chart in the previous activity:

```{r eval = FALSE}
titanic %>%
  ggplot(aes(x = class)) +
  geom_bar()
```

Describe in your own words at least 2 similarities and at least 2 differences between the chunk of code from the previous activity and the chunk of code we just used to analyze the Berkeley data.

---

In the plot we just made, the total height of each bar represents the total number of applicants for each department who were either admitted or rejected.  Each bar is divided into two parts in different colors, representing the numbers of male and female applicants in each bar.  You can use the total height of the bars to see the relative number of people who were admitted vs. rejected in each department; you can use the amount of red vs. teal in each panel to see the relative number of female vs. male applicants for each department.

Putting the pieces together, we have found the following:

* Across all departments, the proportion of female applicants admitted is *lower* than the proportion of male applicants admitted.
* Within most departments, the proportion of female applicants admitted is *higher* than the proportion of male applicants admitted.
* Departments with high overall admission rates receive fewer female applicants than departments with low admission rates.

Finally, we should note that, although the names of the specific departments have been removed, departments A and B (which were easy to get into and received predominantly male applicants) were in the physical sciences and engineering whereas departments E and F (which were hard to get into and received considerably more female applicants) were in the social sciences and humanities.

---

**Exercise 7**

Even though these results do not suggest a sex bias in admissions at the level of individual departments, what other kinds of bias do these results suggest might be going on?  How are these potential biases related to the types of sampling biases we've discussed in class and in the book?

---

## Wrap-up

The paradox we confronted in this activity has a name: "**Simpson's Paradox**".  Check out a good video demonstration of the paradox [here](https://youtu.be/ebEkn-BiW5k).

Simpson's Paradox occurs whenever a pattern that appears in aggregate (like the bias against female applicants over all departments) disappears or reverses when we look at different subgroups (like the apparent bias in favor of female applicants within most departments).  As we have done in this activity, Simpson's Paradox can be resolved only by careful exploration of the data and consideration of potential confounding variables that are associated with different subgroups.